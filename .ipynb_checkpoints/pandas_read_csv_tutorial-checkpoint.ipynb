{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Tutorial: Importing Data with read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your CSV file into Python with pandas\n",
    "\n",
    "**The first step to any data science project is to import your data.** Often, you'll work with data in Comma Separated Value (CSV) files and run into problems at the very start of your workflow. In this tutorial, you'll see how you can use the `read_csv()` function from `pandas` to deal with common problems when importing data and see why loading flat files specifically with `pandas` has become standard practice for working data scientists today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The filesystem\n",
    "\n",
    "Before you can use `pandas` to import your data, you need to know where your data file is in your filesystem and what your current working directory is. You'll see why this is important very soon, but let's review some basic concepts and Shell commands, and see how to execute them directly from the IPython console:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything on the computer is stored in the filesystem. 'Directories' is just another word for 'folders', and the working directory is simply the folder you're currently in. Here are some Shell commands to navigate your way in the filesystem:\n",
    "\n",
    "- The `pwd` command prints the path of your current working directory.\n",
    "- The `ls` command lists all the files and sub-directories (i.e. directories in the current working directory).\n",
    "- The `cd` command followed by the name of a sub-directory allows you to change your working directory to the sub-directory you specify.\n",
    "\n",
    "The command line lets people do many things efficiently with just a few keystrokes. Take the [Introduction to Shell for Data Science](https://www.datacamp.com/courses/introduction-to-shell-for-data-science) course to learn its great utility. Starting a line with `!` in the IPython console gives you complete system shell access.\n",
    "\n",
    "In your filesystem, there's a file called `cereal.csv` that contains [nutrition data on 80 cereals](https://www.kaggle.com/crawford/80-cereals) (slightly modified to accommodate the purposes of this tutorial). Using the commands above, can you find the path to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "*Insert DataCamp's Terminal console here for readers to interact with for early engagement*\n",
    "\n",
    "*Like this but with Terminal console embedded if possible, if not, embed the IPython console and access file using `! ls` and `%cd` commands*\n",
    "![](embedded_console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "Great!\n",
    "\n",
    "Now that you know what your current working directory is and where the data is in the filesystem, you can specify the file path to it. You're now ready to import the CSV file into Python using `read_csv()` from `pandas`. Note that the `pandas` library is usually imported under the alias `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cereal_df = pd.read_csv(\"/Users/mm82089/dc/toots/data/cereal.csv\")\n",
    "cereal_df2 = pd.read_csv(\"data/cereal.csv\")\n",
    "\n",
    "# Are they the same?\n",
    "print(pd.DataFrame.equals(cereal_df, cereal_df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the code chunk above, the file path is the first argument to `read_csv()` and it was specified in two ways. You can use the full file path which includes the working directory or just use the relative file path. The `read_csv()` function is smart enough to decipher whether it's working with full or relative file paths and convert your flat file as a DataFrame with ease.\n",
    "\n",
    "Continue on and see how else `pandas` makes importing CSV files easier. Let's use some of the function's customizable options, particularly for the way it deals with headers, incorrect data types, and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with headers\n",
    "\n",
    "Headers refer to the column names of your dataset. For some datasets you might encounter, the headers may be completely missing, partially missing, or they may exist, but you may want to rename them. How do we deal with such issues effectively?\n",
    "\n",
    "Let's take a closer look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         X.1      X.2      X.3       X.4      X.5      X.6  \\\n",
      "0                       name      mfr     type  calories  protein      fat   \n",
      "1                  100% Bran        N        C        70        4        1   \n",
      "2          100% Natural Bran        Q  no info       120        3        5   \n",
      "3                   All-Bran  no info        C        70        4        1   \n",
      "4  All-Bran with Extra Fiber        K        C        50        4  no info   \n",
      "\n",
      "      X.7    X.8      X.9    X.10     X.11      X.12   X.13     X.14  X.15  \\\n",
      "0  sodium  fiber    carbo  sugars   potass  vitamins  shelf   weight  cups   \n",
      "1       .     10  no info       6      280        25      3        1  0.33   \n",
      "2      15      2        8       8      135         0      .        1     1   \n",
      "3     260      9        7       5  no info        25      3        1  0.33   \n",
      "4     140     14        8       0      330        25      3  no info   0.5   \n",
      "\n",
      "        X.16  \n",
      "0     rating  \n",
      "1  68.402973  \n",
      "2    no info  \n",
      "3  59.425505  \n",
      "4  93.704912  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/cereal.csv\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the actual column names are `name`, `mfr`, ..., `rating`, but it's incorrectly imported as the first observation in the dataset! The `read_csv()` function has an argument called `skiprows` that allows you to specify the line numbers to skip, or the number of lines to skip at the start of the file. In this case, it seems like you'd want to skip the first line, so let's try importing your CSV file with `skiprows` set equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name      mfr     type  calories protein      fat  \\\n",
      "0                  100% Bran        N        C        70       4        1   \n",
      "1          100% Natural Bran        Q  no info       120       3        5   \n",
      "2                   All-Bran  no info        C        70       4        1   \n",
      "3  All-Bran with Extra Fiber        K        C        50       4  no info   \n",
      "4             Almond Delight        R        C       110       2        2   \n",
      "\n",
      "  sodium  fiber    carbo  sugars   potass vitamins shelf   weight  cups  \\\n",
      "0      .   10.0  no info       6      280       25     3        1  0.33   \n",
      "1     15    2.0        8       8      135        0     .        1  1.00   \n",
      "2    260    9.0        7       5  no info       25     3        1  0.33   \n",
      "3    140   14.0        8       0      330       25     3  no info  0.50   \n",
      "4    200    1.0       14       8       -1        .     3        1  0.75   \n",
      "\n",
      "      rating  \n",
      "0  68.402973  \n",
      "1    no info  \n",
      "2  59.425505  \n",
      "3  93.704912  \n",
      "4  34.384843  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cereal.csv\", skiprows = 1)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Even when you don't specify the headers, the `read_csv()` function correctly infers that the first observation contains the headers for the dataset. Not only that, `read_csv()` can infer the data types for each column of your dataset as well. For example, the `calories` column is an integer column, whereas the `fiber` column is a float column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(df['calories'].dtypes)\n",
    "print(df['fiber'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values and incorrect data types\n",
    "\n",
    "In `pandas`, columns with a string value are stored as type `object` by default. Because missing values in this dataset appear to be encoded as either `'no info'` or `'.'`, both string values, checking the data type for a column with missing values such as the `fat` column, you can see that its data type isn't ideal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df['fat'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the column's data type is a Python `object`, doing simple arithmetic results in unexpected results. This sort of behavior can be problematic when doing all sorts of tasks—visualizing distributions, finding outliers, training models—because you expect Python to treat numbers as numbers. That is, $5 + 1$ should not be $51$ as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "print(df['fat'][1])\n",
    "print(df['fat'][2])\n",
    "print(df['fat'][1] + df['fat'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fat` column should be treated as type `int64` or `float64`, and missing data should be encoded as `NaN` so that you can apply statistics in a missing-value-friendly manner. Instead of parsing through each column and replacing `'no info'` and `'.'` with `NaN` values after the data is loaded, you can use the `na_values` argument to take care of those before it's loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name  mfr type  calories  protein  fat  sodium  fiber  \\\n",
      "0                  100% Bran    N    C        70      4.0  1.0     NaN   10.0   \n",
      "1          100% Natural Bran    Q  NaN       120      3.0  5.0    15.0    2.0   \n",
      "2                   All-Bran  NaN    C        70      4.0  1.0   260.0    9.0   \n",
      "3  All-Bran with Extra Fiber    K    C        50      4.0  NaN   140.0   14.0   \n",
      "4             Almond Delight    R    C       110      2.0  2.0   200.0    1.0   \n",
      "\n",
      "   carbo  sugars  potass  vitamins  shelf  weight  cups     rating  \n",
      "0    NaN       6   280.0      25.0    3.0     1.0  0.33  68.402973  \n",
      "1    8.0       8   135.0       0.0    NaN     1.0  1.00        NaN  \n",
      "2    7.0       5     NaN      25.0    3.0     1.0  0.33  59.425505  \n",
      "3    8.0       0   330.0      25.0    3.0     NaN  0.50  93.704912  \n",
      "4   14.0       8    -1.0       NaN    3.0     1.0  0.75  34.384843  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cereal.csv\", skiprows = 1, na_values = ['no info', '.'])\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same arithmetic you saw a moment ago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "1.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(df['fat'][1])\n",
    "print(df['fat'][2])\n",
    "print(df['fat'][1] + df['fat'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. The values in the `fat` column are now treated as numerics.\n",
    "\n",
    "## Recap\n",
    "With a single line of code involving `read_csv()` from `pandas`, you:\n",
    "\n",
    "- Located the flat file you want to import from your filesystem\n",
    "- Corrected headers\n",
    "- Dealt with missing values so that they're encoded properly\n",
    "- Corrected data types for every column\n",
    "- Converted a flat file to a `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "\n",
    "Although the CSV file is one of the most common formats for storing data, there are many other file types that the modern day data scientist must be familiar with. You now have a good sense of how useful `pandas` is when importing the CSV file, and conveniently, `pandas` offers other similar and equally handy functions to import files in Excel, SAS, and Stata formats.\n",
    "\n",
    "Yet there are more essential techniques to master. Due to the active community in open source software, there is constant activity in file formats and ways to import data. Lots of useful, high quality data are hosted on the web and accessed through APIs. If you're curious and want to know the state of the art, the [Importing Data in Python](https://www.datacamp.com/courses/importing-data-in-python-part-1) course series on DataCamp will teach you best practices of importing all kinds of data into Python.\n",
    "\n",
    "Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
