{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Tutorial: Importing Data with read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading your CSV file into Python with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first step to any data science project is to import your data. Often, you'll work with data in Comma Separated Value (CSV) files. Learn how to use `pandas` to convert CSV files seemlessly into Python as `pandas` DataFrames, so that you can start your analysis right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use `pandas` to import our data, we need to know where our data file is in our filesystem and what our current working directory is. You'll see why this is important very soon, but let's review some basic concepts and Shell commands so that we can locate the data file. In this tutorial, we'll be importing nutrition data on 80 cereals into Python (Source: [Kaggle](https://www.kaggle.com/crawford/80-cereals))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything on the computer is stored in the filesystem. 'Directories' is just another word for 'folders'. The working directory is simply the folder you're currently in. Here are some Shell commands you can use to navigate your way in the filesystem:\n",
    "\n",
    "- The `pwd` command prints the path of your current working directory.\n",
    "- The `ls` command lists all the files and sub-directories (i.e. directories in the current working directory).\n",
    "- The `cd` command followed by the name of a sub-directory allows you to change your working directory to the sub-directory you specify.\n",
    "\n",
    "Using these commands, can you locate the `cereal.csv` file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "*Insert DataCamp's Terminal console here for readers to interact with for early engagement*\n",
    "\n",
    "*Like this but with Terminal console embedded if possible, if not, embed the IPython console and access file using `! ls` and `%cd` commands*\n",
    "![](embedded_console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "Great! Now that we know what our current working directory is and where the data is in the filesystem, we can specify the file path to it. Let's import the CSV file into Python using `read_csv()` from `pandas`. Note that the `pandas` library is usually imported under the alias `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cereal_df = pd.read_csv(\"/Users/mm82089/dc/toots/data/cereal.csv\")\n",
    "cereal_df2 = pd.read_csv(\"data/cereal.csv\")\n",
    "\n",
    "# Are they the same?\n",
    "print(pd.DataFrame.equals(cereal_df, cereal_df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the code chunk above, the file path is the first argument to `read_csv()` and it was specified in two ways. You can use the full file path which includes the working directory or just use the relative file path. The `read_csv()` function is smart enough to decipher whether it's working with full or relative file paths and convert your flat file as a dataframe with ease.\n",
    "\n",
    "In the following sections, we will see how else `pandas` increases the utility in importing CSV files. We'll learn some of the ways we can customize the `read_csv()` function, in the way that it deals with headers and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headers refer to the column names of your dataset. For some datasets you might encounter, the headers may be completely missing, partially missing, or they may exist, but you may want to rename them. How do we deal with such issues effectively?\n",
    "\n",
    "Let's take a closer look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         X.1      X.2      X.3       X.4      X.5      X.6  \\\n",
      "0                       name      mfr     type  calories  protein      fat   \n",
      "1                  100% Bran        N        C        70        4        1   \n",
      "2          100% Natural Bran        Q  no info       120        3        5   \n",
      "3                   All-Bran  no info        C        70        4        1   \n",
      "4  All-Bran with Extra Fiber        K        C        50        4  no info   \n",
      "\n",
      "      X.7    X.8      X.9    X.10     X.11      X.12   X.13     X.14  X.15  \\\n",
      "0  sodium  fiber    carbo  sugars   potass  vitamins  shelf   weight  cups   \n",
      "1     130     10  no info       6      280        25      3        1  0.33   \n",
      "2      15      2        8       8      135         0      3        1     1   \n",
      "3     260      9        7       5  no info        25      3        1  0.33   \n",
      "4     140     14        8       0      330        25      3  no info   0.5   \n",
      "\n",
      "        X.16  \n",
      "0     rating  \n",
      "1  68.402973  \n",
      "2    no info  \n",
      "3  59.425505  \n",
      "4  93.704912  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/cereal.csv\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the actual column names are `name`, `mfr`, ..., `rating`, but it's incorrectly imported as the first observation in the dataset! Conveniently, the `read_csv()` function has an argument called `skiprows` that allows you to specify the line numbers to skip (note: it's 0-indexed), or the number of lines to skip at the start of the file. In this case, it seems like we want to skip the first line, so let's try importing our CSV file with `skiprows` set equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name      mfr     type  calories  protein      fat  \\\n",
      "0                  100% Bran        N        C        70        4        1   \n",
      "1          100% Natural Bran        Q  no info       120        3        5   \n",
      "2                   All-Bran  no info        C        70        4        1   \n",
      "3  All-Bran with Extra Fiber        K        C        50        4  no info   \n",
      "4             Almond Delight        R        C       110        2        2   \n",
      "\n",
      "   sodium  fiber    carbo  sugars   potass  vitamins  shelf   weight  cups  \\\n",
      "0     130   10.0  no info       6      280        25      3        1  0.33   \n",
      "1      15    2.0        8       8      135         0      3        1  1.00   \n",
      "2     260    9.0        7       5  no info        25      3        1  0.33   \n",
      "3     140   14.0        8       0      330        25      3  no info  0.50   \n",
      "4     200    1.0       14       8       -1        25      3        1  0.75   \n",
      "\n",
      "      rating  \n",
      "0  68.402973  \n",
      "1    no info  \n",
      "2  59.425505  \n",
      "3  93.704912  \n",
      "4  34.384843  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cereal.csv\", skiprows = 1)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Even when we haven't specified the headers, the `read_csv()` function correctly infers that the first observation contains the headers for the dataset. Not only that, `read_csv()` infers the data types for each column as well. For example, the `calories` column is an integer column, where as the `fiber` column is a float column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(df['calories'].dtypes)\n",
    "print(df['fiber'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, in `pandas`, columns with a string value are stored as type `object`. Because missing data here are encoded as `'no info'`, a string, if we check the data type for `fat`, a numeric column with missing values, we can see that its data type isn't ideal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df['fat'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fat` column should be treated as type `int64`, and we want to recognize missing data as `NaN`. Instead of parsing through each column and replacing `'no info'` with `NaN` values after the data is loaded, we can customize the `read_csv()` function to take care of this before it's loaded using the `na_values` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name  mfr type  calories  protein  fat  sodium  fiber  \\\n",
      "0                  100% Bran    N    C        70        4  1.0     130   10.0   \n",
      "1          100% Natural Bran    Q  NaN       120        3  5.0      15    2.0   \n",
      "2                   All-Bran  NaN    C        70        4  1.0     260    9.0   \n",
      "3  All-Bran with Extra Fiber    K    C        50        4  NaN     140   14.0   \n",
      "4             Almond Delight    R    C       110        2  2.0     200    1.0   \n",
      "\n",
      "   carbo  sugars  potass  vitamins  shelf  weight  cups     rating  \n",
      "0    NaN       6   280.0        25      3     1.0  0.33  68.402973  \n",
      "1    8.0       8   135.0         0      3     1.0  1.00        NaN  \n",
      "2    7.0       5     NaN        25      3     1.0  0.33  59.425505  \n",
      "3    8.0       0   330.0        25      3     NaN  0.50  93.704912  \n",
      "4   14.0       8    -1.0        25      3     1.0  0.75  34.384843  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/cereal.csv\", skiprows = 1, na_values = 'no info')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now our data looks clean and all it took was one line of code with `pandas`'s `read_csv()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts\n",
    "There are other types of data files such as SAS, Excel, Stata you can import into Python using `pandas`. You can learn all the best practices of importing all kinds of data into Python in the [Importing Data in Python](https://www.datacamp.com/courses/importing-data-in-python-part-1) course series on DataCamp. Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(pd.read_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
